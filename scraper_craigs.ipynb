{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to get a call request on the site\n",
    "import requests\n",
    "#For working with data frames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " #for storing data as a csv\n",
    "import csv\n",
    "# web scraping library \n",
    "from bs4 import BeautifulSoup\n",
    "#time management\n",
    "import time\n",
    "from random import randint\n",
    "# regular expression\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of city names we want to get data for\n",
    "cities = ['dallas','chicago', 'newyork', 'sfbay', 'losangeles', \\\n",
    "        'houston', 'phoenix', 'philadelphia', 'sanantonio', 'washingtondc',\\\n",
    "       'boston', 'nashville', 'atlanta', 'miami', 'seattle']\n",
    "\n",
    "# all pages in a city will be stored in this list\n",
    "list_of_cities = []\n",
    "\n",
    "for city in cities:\n",
    "    #say each city has approximately 1800 pages for the \"cars for sale by owner\" category\n",
    "    #we'll track these pages with page_number variable below\n",
    "    page_number = 1\n",
    "\n",
    "    #cycling through the 1800 pages\n",
    "    while page_number <=1800:\n",
    "        #city_link variable takes a different city name from cities everytime through the loop\n",
    "        city_link = \"https://\" + str(city) + \".craigslist.org/d/cars-trucks-by-owner/search/cto?s=\" + \\\n",
    "        str(page_number) + \"&hasPic=1\"\n",
    "        \n",
    "        list_of_cities.append(city_link)\n",
    "        page_number += 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://dallas.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://dallas.craigslist.org/d/cars-trucks-by-owner/search/cto?s=121&hasPic=1',\n",
       " 'https://dallas.craigslist.org/d/cars-trucks-by-owner/search/cto?s=241&hasPic=1',\n",
       " 'https://dallas.craigslist.org/d/cars-trucks-by-owner/search/cto?s=361&hasPic=1',\n",
       " 'https://dallas.craigslist.org/d/cars-trucks-by-owner/search/cto?s=481&hasPic=1']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_cities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "#URLs counter\n",
    "car_urls = 1\n",
    "for each_city_page in list_of_cities:\n",
    "    links_in_each_city_page = requests.get(each_city_page)\n",
    "    #parse the html object from the page to BS object\n",
    "    soup = BeautifulSoup(links_in_each_city_page.content, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        #get the macro-container for the car posts for that page\n",
    "        posts = soup.find_all('a',class_='result-image gallery')\n",
    "\n",
    "        #get all the html links in the page and append them to a list\n",
    "        for link in posts:\n",
    "            l = link.get('href')\n",
    "            links.append(l)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # this code just helps keep count of the looping process\n",
    "if car_urls % 5 == 0:\n",
    "    city_link = l.strip()\n",
    "    start = city_link.find(\"//\") + len(\"//\")\n",
    "    end = city_link.find(\".\")\n",
    "    city_string = city_link[start:end]\n",
    "    print('Number of pages returned ->' + str(car_urls) + '---' + city_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sleep timer to manage our server requests\n",
    "time.sleep(randint(0,1))\n",
    "car_urls += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save links to csv\n",
    "df = pd.DataFrame(links)\n",
    "df.to_csv(\"./links.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links returned --> 6085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1       https://chicago.craigslist.org/nwc/cto/d/bartl...\n",
       "2       https://chicago.craigslist.org/chc/cto/d/chica...\n",
       "3       https://chicago.craigslist.org/nch/cto/d/chica...\n",
       "4       https://chicago.craigslist.org/nwc/cto/d/elgin...\n",
       "5       https://chicago.craigslist.org/chc/cto/d/chica...\n",
       "                              ...                        \n",
       "6081    https://miami.craigslist.org/mdc/cto/d/miami-b...\n",
       "6082    https://miami.craigslist.org/pbc/cto/d/boca-ra...\n",
       "6083    https://miami.craigslist.org/brw/cto/d/fort-la...\n",
       "6084    https://miami.craigslist.org/mdc/cto/d/douglas...\n",
       "6085    https://miami.craigslist.org/brw/cto/d/fort-la...\n",
       "Name: https, Length: 6085, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read links from csv\n",
    "links = pd.read_csv('./links.csv', names=['https'])\n",
    "links = links['https'][1:]\n",
    "print(\"links returned -->\", len(links))\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a counter\n",
    "count = 0\n",
    "\n",
    "# store vehicle details in this list\n",
    "cars = []\n",
    "\n",
    "# loop over all links in the list \n",
    "for link in links:\n",
    "    # make HTTP requests\n",
    "    each_page = requests.get(link)\n",
    "    # The sleep function can help avoid server overload with too many requests in a short period of time\n",
    "    time.sleep(randint(1,2))\n",
    "    # store the BS object in a variable\n",
    "    page_soup = BeautifulSoup(each_page.content, 'html.parser')\n",
    "\n",
    "    # loop over each link and store address\n",
    "    car_details = []\n",
    "    try:\n",
    "        # find price attribute and store in car details\n",
    "        car_details.append(page_soup.find('span', class_=\"price\").text)\n",
    "\n",
    "        # find date and time and append to car details \n",
    "        for span in page_soup.find_all('span', recursive=True):\n",
    "            if not span.attrs.values():\n",
    "                car_details.append(span.text)\n",
    "        car_details.append(\"date time: \" + page_soup.find('time', class_=\"date timeago\")\\\n",
    "                                         .text.strip().replace(':',';'))\n",
    "\n",
    "        # find date and city name and append to car details\n",
    "        city = link.strip()\n",
    "        start = city.find(\"//\") + len(\"//\")\n",
    "        end = city.find(\".\")\n",
    "        substring = city[start:end]\n",
    "        car_details.append('city:' + substring)\n",
    "\n",
    "        # find geo coordinates and append\n",
    "        geos = page_soup.findAll(\"div\",{\"class\": \"mapbox\"})\n",
    "        lat = geos[0].contents[1].get('data-latitude')\n",
    "        car_details.append('lat:' + lat.strip())\n",
    "        long = geos[0].content[1].get('data-longitude')\n",
    "        car_details.append('long:' + long.strip())\n",
    "\n",
    "        # find post body and append\n",
    "        post_body = page_soup.find(attrs={'id' : 'postingbody'}).contents[2]\n",
    "        # remove non ascii characters from post body\n",
    "        car_details.append('post_body:' + re.sub(\"[^0-9a-zA-Z]+\", \" \", post_body))\n",
    "        # find postID and append to car details / We'll use this to assign labels to images later \n",
    "        car_details.append('pID:' + link.strip().replace('html', '').replace('.', '').split('/')[-1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # perform some basic cleanup and store in clean\n",
    "clean = []\n",
    "for string in car_details:\n",
    "    # this attribute came without a label. Assign one.\n",
    "    if string == car_details[1]:\n",
    "        clean.append('year make model: ' + string)\n",
    "    # clean up price text from $9,999 --> 9999\n",
    "    if string == car_details[0]:\n",
    "        clean.append('price: ' + string.replace(',','').replace('$', ''))\n",
    "    else :\n",
    "        clean.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some attributes come without labels. Drop these\n",
    "car_final = []\n",
    "for s in clean:\n",
    "    if ':' in s:\n",
    "        car_final.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append clean attributes for each vehicle to car list\n",
    "cars.append(car_final)\n",
    "count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a counter to keep track of the loop \n",
    "if count % 100 == 0:\n",
    "    print('loop # -> ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to strip() the keys and values after splitting in order to trim white-space\n",
    "def list_to_dict(rlist):\n",
    "    return dict(map(lambda s : map(str.strip, s.split(':')), rlist))\n",
    "\n",
    "# create a dictionary for label:value for each car attribute\n",
    "car_dicts = []\n",
    "for car in cars:\n",
    "    car_dict = list_to_dict(car)\n",
    "    car_dicts.append(car_dict)\n",
    "\n",
    "for i in car_dicts:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Empty DataFrame object\n",
    "dfs = pd.DataFrame()\n",
    "\n",
    "for item in car_dicts:\n",
    "    df = pd.DataFrame.from_dict(item, orient='index').transpose()\n",
    "    # concatenante each new df from the loop into the parent df\n",
    "    dfs = pd.concat([dfs,df], axis=0, ignore_index=True, sort=True)\n",
    "    # clean duplicate year in 'year make model'\n",
    "    dfs['year_c make model'] = dfs['year make model'].str.replace(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data frame to csv\n",
    "dfs.to_csv('car_data.csv', sep='\\t', encoding='utf-8')\n",
    "dfs.to_csv('/Users/keithbaranga/Documents/GitHub/craiglist_scraper/car_data.csv', sep='\\t', encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
