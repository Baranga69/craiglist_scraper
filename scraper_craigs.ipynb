{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to get a call request on the site\n",
    "import requests\n",
    "#For working with data frames\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " #for storing data as a csv\n",
    "import csv\n",
    "# web scraping library \n",
    "from bs4 import BeautifulSoup\n",
    "#time management\n",
    "import time\n",
    "from random import randint\n",
    "# regular expression\n",
    "import re\n",
    "import cchardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of city names we want to get data for\n",
    "cities = ['dallas','chicago', 'newyork', 'sfbay', 'losangeles', \\\n",
    "        'houston', 'phoenix', 'philadelphia', 'sanantonio', 'washingtondc',\\\n",
    "       'boston', 'nashville', 'atlanta', 'miami', 'seattle']\n",
    "\n",
    "# all pages in a city will be stored in this list\n",
    "list_of_cities = []\n",
    "\n",
    "for city in cities:\n",
    "    #say each city has approximately 1800 pages for the \"cars for sale by owner\" category\n",
    "    #we'll track these pages with page_number variable below\n",
    "    page_number = 1\n",
    "\n",
    "    #cycling through the 1800 pages\n",
    "    while page_number <=100:\n",
    "        #city_link variable takes a different city name from cities everytime through the loop\n",
    "        city_link = \"https://\" + str(city) + \".craigslist.org/d/cars-trucks-by-owner/search/cto?s=\" + \\\n",
    "        str(page_number) + \"&hasPic=1\"\n",
    "        \n",
    "        list_of_cities.append(city_link)\n",
    "        page_number += 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of city names we want to get data for\n",
    "cities = ['dallas','chicago', 'newyork', 'sfbay', 'losangeles', \\\n",
    "        'houston', 'phoenix', 'philadelphia', 'sanantonio', 'washingtondc',\\\n",
    "       'boston', 'nashville', 'atlanta', 'miami', 'seattle']\n",
    "\n",
    "# all pages in a city will be stored in this list\n",
    "list_of_links = []\n",
    "for city_link in cities:\n",
    "    #say each city has approximately 1800 pages for the \"cars for sale by owner\" category\n",
    "    #we'll track these pages with page_number variable below\n",
    "    page_number = 1\n",
    "\n",
    "    #cycling through the 1800 pages\n",
    "    while page_number <=100:\n",
    "        #city_link variable takes a different city name from cities everytime through the loop\n",
    "        test_link = \"https://\"+ str(city) + \".craigslist.org/search/ggg?is_paid=yes&query=focus%20group&sort=date#search=1~thumb~0~0\"\n",
    "        \n",
    "        list_of_links.append(test_link)\n",
    "        page_number += 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://seattle.craigslist.org/search/ggg?is_paid=yes&query=focus%20group&sort=date#search=1~thumb~0~0',\n",
       " 'https://seattle.craigslist.org/search/ggg?is_paid=yes&query=focus%20group&sort=date#search=1~thumb~0~0',\n",
       " 'https://seattle.craigslist.org/search/ggg?is_paid=yes&query=focus%20group&sort=date#search=1~thumb~0~0',\n",
       " 'https://seattle.craigslist.org/search/ggg?is_paid=yes&query=focus%20group&sort=date#search=1~thumb~0~0',\n",
       " 'https://seattle.craigslist.org/search/ggg?is_paid=yes&query=focus%20group&sort=date#search=1~thumb~0~0']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://dallas.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://chicago.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://newyork.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://sfbay.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://losangeles.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://houston.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://phoenix.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://philadelphia.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://sanantonio.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1',\n",
       " 'https://washingtondc.craigslist.org/d/cars-trucks-by-owner/search/cto?s=1&hasPic=1']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_cities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "#URLs counter\n",
    "car_urls = 1\n",
    "for each_city_page in list_of_cities:\n",
    "    links_in_each_city_page = requests.get(each_city_page)\n",
    "    #parse the html object from the page to BS object\n",
    "    soup = BeautifulSoup(links_in_each_city_page.content,'lxml')\n",
    "\n",
    "    try:\n",
    "        #get the macro-container for the car posts for that page\n",
    "        posts = soup.find_all('a',class_='result-image gallery')\n",
    "\n",
    "        #get all the html links in the page and append them to a list\n",
    "        for link in posts:\n",
    "            l = link.get('href')\n",
    "            links.append(l)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # this code just helps keep count of the looping process\n",
    "if car_urls % 5 == 0:\n",
    "    city_link = l.strip()\n",
    "    start = city_link.find(\"//\") + len(\"//\")\n",
    "    end = city_link.find(\".\")\n",
    "    city_string = city_link[start:end]\n",
    "    print('Number of pages returned ->' + str(car_urls) + '---' + city_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sleep timer to manage our server requests\n",
    "time.sleep(randint(0,1))\n",
    "car_urls += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save links to csv\n",
    "df = pd.DataFrame(links)\n",
    "df.to_csv(\"./links.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links returned --> 480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1      https://chicago.craigslist.org/sox/cto/d/chica...\n",
       "2      https://chicago.craigslist.org/wcl/cto/d/downe...\n",
       "3      https://chicago.craigslist.org/wcl/cto/d/downe...\n",
       "4      https://chicago.craigslist.org/wcl/cto/d/downe...\n",
       "5      https://chicago.craigslist.org/nch/cto/d/victo...\n",
       "                             ...                        \n",
       "476    https://miami.craigslist.org/pbc/cto/d/boynton...\n",
       "477    https://miami.craigslist.org/pbc/cto/d/boynton...\n",
       "478    https://miami.craigslist.org/mdc/cto/d/hialeah...\n",
       "479    https://miami.craigslist.org/brw/cto/d/pompano...\n",
       "480    https://miami.craigslist.org/mdc/cto/d/hialeah...\n",
       "Name: https, Length: 480, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read links from csv\n",
    "links = pd.read_csv('./links.csv', names=['https'])\n",
    "links = links['https'][1:]\n",
    "print(\"links returned -->\", len(links))\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 links returned --> 9\n"
     ]
    }
   ],
   "source": [
    "first10 = pd.read_csv('./links.csv', names=['https'], nrows=10)\n",
    "first10 = first10['https'][1:]\n",
    "print(\"first 10 links returned -->\", len(first10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop # ->  100\n",
      "loop # ->  200\n",
      "loop # ->  300\n",
      "loop # ->  400\n"
     ]
    }
   ],
   "source": [
    "# justa counter\n",
    "count = 0\n",
    "\n",
    "# store vehicle details in this list\n",
    "cars = []\n",
    "\n",
    "#loop over all links in the list     \n",
    "for link in links:\n",
    "    # make HTTP requests\n",
    "    each_page = requests.get(link)\n",
    "    # The sleep function can help you to avoid the server to be overloaded with too many requests in a very short period of time.\n",
    "    time.sleep(randint(1,2))\n",
    "    # store the BS object in a variable\n",
    "    page_soup = BeautifulSoup(each_page.content, 'lxml')\n",
    "\n",
    "    # loop over each link and store car details\n",
    "    car_details = []\n",
    "    try:\n",
    "        # find price attribute and store in car details\n",
    "        car_details.append(page_soup.find('span', class_=\"price\").text)\n",
    "\n",
    "        # find date time and append to car details\n",
    "        for span in page_soup.find_all('span', recursive=True):\n",
    "            if not span.attrs.values():\n",
    "                car_details.append(span.text)\n",
    "        car_details.append(\"date time: \" + page_soup.find('time', class_=\"date timeago\")\\\n",
    "                                         .text.strip().replace(':',';'))\n",
    "\n",
    "        # find date city name and append to car details\n",
    "        city = link.strip()\n",
    "        start = city.find(\"//\") + len(\"//\")\n",
    "        end = city.find(\".\")\n",
    "        substring = city[start:end]\n",
    "        car_details.append('city:' + substring)\n",
    "\n",
    "        # find geo coordinates and append to car details\n",
    "        geos = page_soup.findAll(\"div\", {\"class\": \"mapbox\"})\n",
    "        lat = geos[0].contents[1].get('data-latitude')\n",
    "        car_details.append('lat:' + lat.strip())\n",
    "        long = geos[0].contents[1].get('data-longitude')\n",
    "        car_details.append('long:' + long.strip())\n",
    "\n",
    "        # find post body and append to car details\n",
    "        post_body = page_soup.find(attrs={'id' : 'postingbody'}).contents[2]\n",
    "        # remove non ascii characters from post bosy\n",
    "        car_details.append('post_body:' + re.sub(\"[^0-9a-zA-Z]+\", \" \", post_body))\n",
    "\n",
    "        # find postID and append to car details / We'll use this to assign labels to images later\n",
    "        car_details.append('pID:' + link.strip().replace('html','').replace('.','').split('/')[-1])\n",
    "    except:\n",
    "        pass    \n",
    "    \n",
    "    # perform some basic cleanup and store in clean\n",
    "    clean = []\n",
    "    for string in car_details:\n",
    "        # this attribute came without a label. Assign one.\n",
    "        if string == car_details[1]:\n",
    "            clean.append('year make model: ' + string)\n",
    "        # clean up price text from $9,999 --> 9999\n",
    "        if string == car_details[0]:\n",
    "            clean.append('price: ' + string.replace(',','').replace('$',''))\n",
    "        else:\n",
    "            clean.append(string)\n",
    "\n",
    "            \n",
    "    # some attributes came without labels. Drop those.\n",
    "    car_final = []\n",
    "    for s in clean:\n",
    "        if ':' in s:\n",
    "            car_final.append(s)\n",
    "            \n",
    "    # append clean attributes for each vehicle to car list\n",
    "    cars.append(car_final)\n",
    "    count += 1\n",
    "\n",
    "    # just a counter to keep track of the loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price: 3700', 'year make model: 2004 toyota corolla', 'condition: like new', 'cylinders: 4 cylinders', 'fuel: gas', 'odometer: 129000', 'paint color: silver', 'size: full-size', 'title status: clean', 'transmission: automatic', 'type: sedan', 'date time: 2022-12-23 09;55', 'city:miami', 'lat:25.832500', 'long:-80.280800', 'post_body: Muy buenas condiciones t tulo limpio venta privada muy buenas condiciones interesados llamar al 786 553 53 23', 'pID:7571235860']\n"
     ]
    }
   ],
   "source": [
    "print(car_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if count % 100 == 0:\n",
    "    print('loop # -> ',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a counter\n",
    "count = 0\n",
    "\n",
    "# store vehicle details in this list\n",
    "cars = []\n",
    "\n",
    "# loop over all links in the list \n",
    "for link in first10:\n",
    "    # make HTTP requests\n",
    "    each_page = requests.get(link)\n",
    "    # The sleep function can help avoid server overload with too many requests in a short period of time\n",
    "    time.sleep(randint(1,2))\n",
    "    # store the BS object in a variable\n",
    "    page_soup = BeautifulSoup(each_page.content, 'lxml')\n",
    "\n",
    "    # loop over each link and store address\n",
    "    car_details = []\n",
    "    try:\n",
    "        # find price attribute and store in car details\n",
    "        car_details.append(page_soup.find('span', class_=\"price\").text)\n",
    "\n",
    "        # find date and time and append to car details \n",
    "        for span in page_soup.find_all('span', recursive=True):\n",
    "            if not span.attrs.values():\n",
    "                car_details.append(span.text)\n",
    "        car_details.append(\"date time: \" + page_soup.find('time', class_=\"date timeago\")\\\n",
    "                                            .text.strip().replace(':',';'))\n",
    "\n",
    "        # find date and city name and append to car details\n",
    "        city = link.strip()\n",
    "        start = city.find(\"//\") + len(\"//\")\n",
    "        end = city.find(\".\")\n",
    "        substring = city[start:end]\n",
    "        car_details.append('city:' + substring)\n",
    "\n",
    "        # find geo coordinates and append\n",
    "        geos = page_soup.findAll(\"div\",{\"class\": \"mapbox\"})\n",
    "        lat = geos[0].contents[1].get('data-latitude')\n",
    "        car_details.append('lat:' + lat.strip())\n",
    "        long = geos[0].content[1].get('data-longitude')\n",
    "        car_details.append('long:' + long.strip())\n",
    "\n",
    "        # find post body and append\n",
    "        post_body = page_soup.find(attrs={'id' : 'postingbody'}).contents[2]\n",
    "        # remove non ascii characters from post body\n",
    "        car_details.append('post_body:' + re.sub(\"[^0-9a-zA-Z]+\", \" \", post_body))\n",
    "        # find postID and append to car details / We'll use this to assign labels to images later \n",
    "        car_details.append('pID:' + link.strip().replace('html', '').replace('.', '').split('/')[-1])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$12,900', '2013 Lincoln MKX', 'condition: excellent', 'fuel: gas', 'odometer: 121000', 'title status: clean', 'transmission: automatic', 'date time: 2022-12-23 17;44', 'city:chicago', 'lat:41.837100']\n"
     ]
    }
   ],
   "source": [
    "print(car_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    " # perform some basic cleanup and store in clean\n",
    "clean = []\n",
    "for string in car_details:\n",
    "    # this attribute came without a label. Assign one.\n",
    "    if string == car_details[1]:\n",
    "        clean.append('year make model: ' + string)\n",
    "    # clean up price text from $9,999 --> 9999\n",
    "    if string == car_details[0]:\n",
    "        clean.append('price: ' + string.replace(',','').replace('$', ''))\n",
    "    else :\n",
    "        clean.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some attributes come without labels. Drop these\n",
    "car_final = []\n",
    "for s in clean:\n",
    "    if ':' in s:\n",
    "        car_final.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price: 12900', 'year make model: 2013 Lincoln MKX', 'condition: excellent', 'fuel: gas', 'odometer: 121000', 'title status: clean', 'transmission: automatic', 'date time: 2022-12-23 17;44', 'city:chicago', 'lat:41.837100']\n"
     ]
    }
   ],
   "source": [
    "# append clean attributes for each vehicle to car list\n",
    "cars.append(car_final)\n",
    "count += 1\n",
    "print(car_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a counter to keep track of the loop \n",
    "if count % 100 == 0:\n",
    "    print('loop # -> ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'price': '12900', 'year make model': '2013 Lincoln MKX', 'condition': 'excellent', 'fuel': 'gas', 'odometer': '121000', 'title status': 'clean', 'transmission': 'automatic', 'date time': '2022-12-23 17;44', 'city': 'chicago', 'lat': '41.837100'}\n"
     ]
    }
   ],
   "source": [
    "# method to strip() the keys and values after splitting in order to trim white-space\n",
    "def list_to_dict(rlist):\n",
    "    return dict(map(lambda s : map(str.strip, s.split(':')), rlist))\n",
    "\n",
    "# create a dictionary for label:value for each car attribute\n",
    "car_dicts = []\n",
    "for car in cars:\n",
    "    car_dict = list_to_dict(car)\n",
    "    car_dicts.append(car_dict)\n",
    "\n",
    "for i in car_dicts:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Empty DataFrame object\n",
    "dfs = pd.DataFrame()\n",
    "\n",
    "for item in car_dicts:\n",
    "    df = pd.DataFrame.from_dict(item, orient='index').transpose()\n",
    "    # concatenante each new df from the loop into the parent df\n",
    "    dfs = pd.concat([dfs,df], axis=0, ignore_index=True, sort=True)\n",
    "    # clean duplicate year in 'year make model'\n",
    "    dfs['year_c make model'] = dfs['year make model'].str.replace(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['city', 'condition', 'date time', 'fuel', 'lat', 'odometer', 'price',\n",
      "       'title status', 'transmission', 'year make model', 'year_c make model'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>condition</th>\n",
       "      <th>date time</th>\n",
       "      <th>fuel</th>\n",
       "      <th>lat</th>\n",
       "      <th>odometer</th>\n",
       "      <th>price</th>\n",
       "      <th>title status</th>\n",
       "      <th>transmission</th>\n",
       "      <th>year make model</th>\n",
       "      <th>year_c make model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chicago</td>\n",
       "      <td>excellent</td>\n",
       "      <td>2022-12-23 17;44</td>\n",
       "      <td>gas</td>\n",
       "      <td>41.837100</td>\n",
       "      <td>121000</td>\n",
       "      <td>12900</td>\n",
       "      <td>clean</td>\n",
       "      <td>automatic</td>\n",
       "      <td>2013 Lincoln MKX</td>\n",
       "      <td>2013 Lincoln MKX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city  condition         date time fuel        lat odometer  price  \\\n",
       "0  chicago  excellent  2022-12-23 17;44  gas  41.837100   121000  12900   \n",
       "\n",
       "  title status transmission   year make model year_c make model  \n",
       "0        clean    automatic  2013 Lincoln MKX  2013 Lincoln MKX  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dfs.columns)\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data frame to csv\n",
    "dfs.to_csv('car_data.csv', sep='\\t', encoding='utf-8')\n",
    "dfs.to_csv('/Users/keithbaranga/Documents/GitHub/craiglist_scraper/car_data.csv', sep='\\t', encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
